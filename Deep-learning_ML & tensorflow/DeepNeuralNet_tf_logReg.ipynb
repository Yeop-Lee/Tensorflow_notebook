{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "Run 0,0.319999992847\n",
      "Run 1,0.319999992847\n",
      "Run 2,0.319999992847\n",
      "Run 3,0.319999992847\n",
      "Run 4,0.319999992847\n",
      "Run 5,0.319999992847\n",
      "Run 6,0.319999992847\n",
      "Run 7,0.319999992847\n",
      "Run 8,0.319999992847\n",
      "Run 9,0.319999992847\n",
      "Run 10,0.319999992847\n",
      "Run 11,0.319999992847\n",
      "Run 12,0.319999992847\n",
      "Run 13,0.319999992847\n",
      "Run 14,0.340000003576\n",
      "Run 15,0.340000003576\n",
      "Run 16,0.40000000596\n",
      "Run 17,0.439999997616\n",
      "Run 18,0.5\n",
      "Run 19,0.519999980927\n",
      "Run 20,0.639999985695\n",
      "Run 21,0.680000007153\n",
      "Run 22,0.699999988079\n",
      "Run 23,0.699999988079\n",
      "Run 24,0.699999988079\n",
      "Run 25,0.699999988079\n",
      "Run 26,0.699999988079\n",
      "Run 27,0.699999988079\n",
      "Run 28,0.699999988079\n",
      "Run 29,0.699999988079\n",
      "Run 30,0.699999988079\n",
      "Run 31,0.699999988079\n",
      "Run 32,0.699999988079\n",
      "Run 33,0.699999988079\n",
      "Run 34,0.699999988079\n",
      "Run 35,0.699999988079\n",
      "Run 36,0.699999988079\n",
      "Run 37,0.699999988079\n",
      "Run 38,0.699999988079\n",
      "Run 39,0.699999988079\n",
      "Run 40,0.699999988079\n",
      "Run 41,0.699999988079\n",
      "Run 42,0.699999988079\n",
      "Run 43,0.699999988079\n",
      "Run 44,0.699999988079\n",
      "Run 45,0.699999988079\n",
      "Run 46,0.699999988079\n",
      "Run 47,0.699999988079\n",
      "Run 48,0.699999988079\n",
      "Run 49,0.699999988079\n",
      "Run 50,0.699999988079\n",
      "Run 51,0.699999988079\n",
      "Run 52,0.699999988079\n",
      "Run 53,0.699999988079\n",
      "Run 54,0.699999988079\n",
      "Run 55,0.699999988079\n",
      "Run 56,0.699999988079\n",
      "Run 57,0.699999988079\n",
      "Run 58,0.699999988079\n",
      "Run 59,0.699999988079\n",
      "Run 60,0.699999988079\n",
      "Run 61,0.699999988079\n",
      "Run 62,0.699999988079\n",
      "Run 63,0.699999988079\n",
      "Run 64,0.699999988079\n",
      "Run 65,0.699999988079\n",
      "Run 66,0.699999988079\n",
      "Run 67,0.699999988079\n",
      "Run 68,0.699999988079\n",
      "Run 69,0.699999988079\n",
      "Run 70,0.699999988079\n",
      "Run 71,0.699999988079\n",
      "Run 72,0.699999988079\n",
      "Run 73,0.699999988079\n",
      "Run 74,0.699999988079\n",
      "Run 75,0.699999988079\n",
      "Run 76,0.699999988079\n",
      "Run 77,0.699999988079\n",
      "Run 78,0.699999988079\n",
      "Run 79,0.699999988079\n",
      "Run 80,0.699999988079\n",
      "Run 81,0.699999988079\n",
      "Run 82,0.699999988079\n",
      "Run 83,0.699999988079\n",
      "Run 84,0.699999988079\n",
      "Run 85,0.699999988079\n",
      "Run 86,0.699999988079\n",
      "Run 87,0.699999988079\n",
      "Run 88,0.699999988079\n",
      "Run 89,0.699999988079\n",
      "Run 90,0.699999988079\n",
      "Run 91,0.699999988079\n",
      "Run 92,0.699999988079\n",
      "Run 93,0.699999988079\n",
      "Run 94,0.699999988079\n",
      "Run 95,0.699999988079\n",
      "Run 96,0.699999988079\n",
      "Run 97,0.699999988079\n",
      "Run 98,0.699999988079\n",
      "Run 99,0.699999988079\n",
      "Run 100,0.699999988079\n",
      "Run 101,0.699999988079\n",
      "Run 102,0.699999988079\n",
      "Run 103,0.699999988079\n",
      "Run 104,0.699999988079\n",
      "Run 105,0.699999988079\n",
      "Run 106,0.699999988079\n",
      "Run 107,0.699999988079\n",
      "Run 108,0.699999988079\n",
      "Run 109,0.699999988079\n",
      "Run 110,0.699999988079\n",
      "Run 111,0.699999988079\n",
      "Run 112,0.699999988079\n",
      "Run 113,0.72000002861\n",
      "Run 114,0.72000002861\n",
      "Run 115,0.72000002861\n",
      "Run 116,0.72000002861\n",
      "Run 117,0.72000002861\n",
      "Run 118,0.72000002861\n",
      "Run 119,0.72000002861\n",
      "Run 120,0.72000002861\n",
      "Run 121,0.72000002861\n",
      "Run 122,0.72000002861\n",
      "Run 123,0.740000009537\n",
      "Run 124,0.740000009537\n",
      "Run 125,0.740000009537\n",
      "Run 126,0.740000009537\n",
      "Run 127,0.740000009537\n",
      "Run 128,0.740000009537\n",
      "Run 129,0.740000009537\n",
      "Run 130,0.740000009537\n",
      "Run 131,0.740000009537\n",
      "Run 132,0.759999990463\n",
      "Run 133,0.759999990463\n",
      "Run 134,0.759999990463\n",
      "Run 135,0.759999990463\n",
      "Run 136,0.759999990463\n",
      "Run 137,0.77999997139\n",
      "Run 138,0.77999997139\n",
      "Run 139,0.77999997139\n",
      "Run 140,0.77999997139\n",
      "Run 141,0.77999997139\n",
      "Run 142,0.77999997139\n",
      "Run 143,0.77999997139\n",
      "Run 144,0.77999997139\n",
      "Run 145,0.77999997139\n",
      "Run 146,0.77999997139\n",
      "Run 147,0.800000011921\n",
      "Run 148,0.800000011921\n",
      "Run 149,0.800000011921\n",
      "Run 150,0.800000011921\n",
      "Run 151,0.800000011921\n",
      "Run 152,0.800000011921\n",
      "Run 153,0.800000011921\n",
      "Run 154,0.800000011921\n",
      "Run 155,0.800000011921\n",
      "Run 156,0.800000011921\n",
      "Run 157,0.819999992847\n",
      "Run 158,0.819999992847\n",
      "Run 159,0.819999992847\n",
      "Run 160,0.819999992847\n",
      "Run 161,0.819999992847\n",
      "Run 162,0.819999992847\n",
      "Run 163,0.819999992847\n",
      "Run 164,0.819999992847\n",
      "Run 165,0.819999992847\n",
      "Run 166,0.819999992847\n",
      "Run 167,0.819999992847\n",
      "Run 168,0.819999992847\n",
      "Run 169,0.819999992847\n",
      "Run 170,0.819999992847\n",
      "Run 171,0.819999992847\n",
      "Run 172,0.819999992847\n",
      "Run 173,0.819999992847\n",
      "Run 174,0.819999992847\n",
      "Run 175,0.819999992847\n",
      "Run 176,0.819999992847\n",
      "Run 177,0.819999992847\n",
      "Run 178,0.819999992847\n",
      "Run 179,0.819999992847\n",
      "Run 180,0.819999992847\n",
      "Run 181,0.819999992847\n",
      "Run 182,0.819999992847\n",
      "Run 183,0.819999992847\n",
      "Run 184,0.819999992847\n",
      "Run 185,0.819999992847\n",
      "Run 186,0.819999992847\n",
      "Run 187,0.819999992847\n",
      "Run 188,0.819999992847\n",
      "Run 189,0.819999992847\n",
      "Run 190,0.819999992847\n",
      "Run 191,0.819999992847\n",
      "Run 192,0.819999992847\n",
      "Run 193,0.819999992847\n",
      "Run 194,0.819999992847\n",
      "Run 195,0.819999992847\n",
      "Run 196,0.819999992847\n",
      "Run 197,0.819999992847\n",
      "Run 198,0.819999992847\n",
      "Run 199,0.819999992847\n",
      "Run 200,0.819999992847\n",
      "Run 201,0.819999992847\n",
      "Run 202,0.819999992847\n",
      "Run 203,0.819999992847\n",
      "Run 204,0.819999992847\n",
      "Run 205,0.819999992847\n",
      "Run 206,0.819999992847\n",
      "Run 207,0.839999973774\n",
      "Run 208,0.839999973774\n",
      "Run 209,0.839999973774\n",
      "Run 210,0.839999973774\n",
      "Run 211,0.839999973774\n",
      "Run 212,0.839999973774\n",
      "Run 213,0.860000014305\n",
      "Run 214,0.860000014305\n",
      "Run 215,0.860000014305\n",
      "Run 216,0.860000014305\n",
      "Run 217,0.860000014305\n",
      "Run 218,0.860000014305\n",
      "Run 219,0.860000014305\n",
      "Run 220,0.860000014305\n",
      "Run 221,0.860000014305\n",
      "Run 222,0.860000014305\n",
      "Run 223,0.860000014305\n",
      "Run 224,0.879999995232\n",
      "Run 225,0.879999995232\n",
      "Run 226,0.879999995232\n",
      "Run 227,0.879999995232\n",
      "Run 228,0.879999995232\n",
      "Run 229,0.879999995232\n",
      "Run 230,0.879999995232\n",
      "Run 231,0.879999995232\n",
      "Run 232,0.879999995232\n",
      "Run 233,0.879999995232\n",
      "Run 234,0.879999995232\n",
      "Run 235,0.879999995232\n",
      "Run 236,0.879999995232\n",
      "Run 237,0.879999995232\n",
      "Run 238,0.879999995232\n",
      "Run 239,0.879999995232\n",
      "Run 240,0.879999995232\n",
      "Run 241,0.879999995232\n",
      "Run 242,0.879999995232\n",
      "Run 243,0.879999995232\n",
      "Run 244,0.879999995232\n",
      "Run 245,0.879999995232\n",
      "Run 246,0.879999995232\n",
      "Run 247,0.879999995232\n",
      "Run 248,0.879999995232\n",
      "Run 249,0.879999995232\n",
      "Run 250,0.879999995232\n",
      "Run 251,0.879999995232\n",
      "Run 252,0.879999995232\n",
      "Run 253,0.879999995232\n",
      "Run 254,0.879999995232\n",
      "Run 255,0.879999995232\n",
      "Run 256,0.879999995232\n",
      "Run 257,0.879999995232\n",
      "Run 258,0.879999995232\n",
      "Run 259,0.879999995232\n",
      "Run 260,0.879999995232\n",
      "Run 261,0.879999995232\n",
      "Run 262,0.879999995232\n",
      "Run 263,0.879999995232\n",
      "Run 264,0.879999995232\n",
      "Run 265,0.879999995232\n",
      "Run 266,0.879999995232\n",
      "Run 267,0.879999995232\n",
      "Run 268,0.879999995232\n",
      "Run 269,0.879999995232\n",
      "Run 270,0.879999995232\n",
      "Run 271,0.879999995232\n",
      "Run 272,0.879999995232\n",
      "Run 273,0.879999995232\n",
      "Run 274,0.879999995232\n",
      "Run 275,0.879999995232\n",
      "Run 276,0.879999995232\n",
      "Run 277,0.879999995232\n",
      "Run 278,0.879999995232\n",
      "Run 279,0.879999995232\n",
      "Run 280,0.879999995232\n",
      "Run 281,0.879999995232\n",
      "Run 282,0.879999995232\n",
      "Run 283,0.879999995232\n",
      "Run 284,0.879999995232\n",
      "Run 285,0.879999995232\n",
      "Run 286,0.879999995232\n",
      "Run 287,0.879999995232\n",
      "Run 288,0.879999995232\n",
      "Run 289,0.879999995232\n",
      "Run 290,0.879999995232\n",
      "Run 291,0.879999995232\n",
      "Run 292,0.879999995232\n",
      "Run 293,0.879999995232\n",
      "Run 294,0.879999995232\n",
      "Run 295,0.879999995232\n",
      "Run 296,0.879999995232\n",
      "Run 297,0.879999995232\n",
      "Run 298,0.879999995232\n",
      "Run 299,0.879999995232\n",
      "<<<<<<DONE>>>>>>\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "## Ricardo A. Calix, PNW, 2016\n",
    "## Code put together from notes from the book\n",
    "## Fundamentals of deep learning (O'Reily)\n",
    "## Book author: Nikhil Buduma\n",
    "##########################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "###########################################################\n",
    "# Build Example Data is CSV format, but use Iris data\n",
    "#from sklearn import datasets\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#import sklearn\n",
    "\n",
    "###########################################################\n",
    "\n",
    "#parameters\n",
    "#learning_rate = 0.01\n",
    "#training_epochs = 1000\n",
    "#batch_size = 100\n",
    "#display_step = 1\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Convert to one hot\n",
    "def convertOneHot(data):\n",
    "    y=np.array([int(i[0]) for i in data])\n",
    "    y_onehot=[0]*len(y)\n",
    "    for i,j in enumerate(y):\n",
    "        y_onehot[i]=[0]*(y.max() + 1)\n",
    "        y_onehot[i][j]=1\n",
    "    return (y,y_onehot)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "data = genfromtxt('cs-training.csv',delimiter=',')  # Training data\n",
    "test_data = genfromtxt('cs-testing.csv',delimiter=',')  # Test data\n",
    "\n",
    "#print 'train', data\n",
    "#x = raw_input()\n",
    "#print 'test', test_data\n",
    "#x = raw_input()\n",
    "\n",
    "############################################################\n",
    "\n",
    "# creates train set just features with no classes\n",
    "x_train=np.array([ i[1::] for i in data])\n",
    "# classes vectors (setosa, virginica, versicolor)\n",
    "y_train,y_train_onehot = convertOneHot(data)\n",
    "\n",
    "# creates test set just features with no classes\n",
    "x_test=np.array([ i[1::] for i in test_data])\n",
    "# classes vectors (setosa, virginica, versicolor)\n",
    "y_test,y_test_onehot = convertOneHot(test_data)\n",
    "\n",
    "###########################################################\n",
    "#features (A) and classes (B)\n",
    "#  A number of features, 4 in this example\n",
    "#  B = 3 species of Iris (setosa, virginica and versicolor)\n",
    "A=data.shape[1]-1 # Number of features, Note first is y\n",
    "B=len(y_train_onehot[0])\n",
    "\n",
    "\n",
    "###########################################################\n",
    "#this works\n",
    "#x = tf.placeholder(tf.float32, name=\"x\", shape=[None, 4])\n",
    "#W = tf.Variable(tf.random_uniform([4, 3], -1, 1), name=\"W\")\n",
    "#b = tf.Variable(tf.zeros([3]), name=\"biases\")\n",
    "#output = tf.matmul(x, W) + b\n",
    "\n",
    "#init_op = tf.initialize_all_variables()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "#feed_dict = { x : x_train }\n",
    "#result = sess.run(output, feed_dict=feed_dict)\n",
    "#print result\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_stddev = (2.0/weight_shape[0])**0.5\n",
    "    w_init = tf.random_normal_initializer(stddev=weight_stddev)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
    "\n",
    "##########################################################\n",
    "#defines network architecture\n",
    "#deep neural network with 2 hidden layers\n",
    "\n",
    "def inference_deep(x, A, B):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [A, 4],[4])\n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [4, 4],[4])\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [4, B], [B])\n",
    "    return output\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def loss_deep(output, y):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(output, y)\n",
    "    loss = tf.reduce_mean(xentropy) \n",
    "    return loss\n",
    "\n",
    "\n",
    "###########################################################\n",
    "#defines the network architecture\n",
    "#simple logistic regression\n",
    "\n",
    "def inference(x, A, B):\n",
    "    W = tf.Variable(tf.zeros([A,B]))\n",
    "    b = tf.Variable(tf.zeros([B]))\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    return output\n",
    "   \n",
    "###########################################################\n",
    "\n",
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)#remove indices?\n",
    "    loss = tf.reduce_mean(xentropy) #remove this line?\n",
    "    return loss\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "def training(cost):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train_op = optimizer.minimize(cost)\n",
    "    return train_op\n",
    "\n",
    "###########################################################\n",
    "## add accuracy checking nodes\n",
    "\n",
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy\n",
    "\n",
    "###########################################################\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, A]) # Features\n",
    "y = tf.placeholder(\"float\", [None,B]) #correct label for x\n",
    "\n",
    "#output = inference_deep(x, A, B) ## for deep NN with 2 hidden layers\n",
    "#cost = loss_deep(output, y)\n",
    "\n",
    "output = inference(x, A, B) ## for logistic regression\n",
    "cost = loss(output, y)\n",
    "\n",
    "train_op = training(cost)\n",
    "eval_op = evaluate(output, y)\n",
    "\n",
    "##################################################################\n",
    "# Initialize and run\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print(\"...\")\n",
    "# Run the training\n",
    "for i in range(300):\n",
    "    sess.run(train_op, feed_dict={x: x_train, y: y_train_onehot})\n",
    "    result = sess.run(eval_op, feed_dict={x: x_test, y: y_test_onehot})\n",
    "    print \"Run {},{}\".format(i,result)\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print \"<<<<<<DONE>>>>>>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
