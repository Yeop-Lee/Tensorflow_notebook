{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "## Ricardo A. Calix, PNW, 2016\n",
    "## Code put together from notes from the book\n",
    "## Fundamentals of deep learning (O'Reily)\n",
    "## Book author: Nikhil Buduma\n",
    "##########################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "###########################################################\n",
    "# Build Example Data is CSV format, but use Iris data\n",
    "#from sklearn import datasets\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#import sklearn\n",
    "\n",
    "###########################################################\n",
    "\n",
    "#parameters\n",
    "#learning_rate = 0.01\n",
    "#training_epochs = 1000\n",
    "#batch_size = 100\n",
    "#display_step = 1\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Convert to one hot\n",
    "def convertOneHot(data):\n",
    "    y=np.array([int(i[0]) for i in data])\n",
    "    y_onehot=[0]*len(y)\n",
    "    for i,j in enumerate(y):\n",
    "        y_onehot[i]=[0]*(y.max() + 1)\n",
    "        y_onehot[i][j]=1\n",
    "    return (y,y_onehot)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "data = genfromtxt('cs-training.csv',delimiter=',')  # Training data\n",
    "test_data = genfromtxt('cs-testing.csv',delimiter=',')  # Test data\n",
    "\n",
    "#print 'train', data\n",
    "#x = raw_input()\n",
    "#print 'test', test_data\n",
    "#x = raw_input()\n",
    "\n",
    "############################################################\n",
    "\n",
    "# creates train set just features with no classes\n",
    "x_train=np.array([ i[1::] for i in data])\n",
    "# classes vectors (setosa, virginica, versicolor)\n",
    "y_train,y_train_onehot = convertOneHot(data)\n",
    "\n",
    "# creates test set just features with no classes\n",
    "x_test=np.array([ i[1::] for i in test_data])\n",
    "# classes vectors (setosa, virginica, versicolor)\n",
    "y_test,y_test_onehot = convertOneHot(test_data)\n",
    "\n",
    "###########################################################\n",
    "#features (A) and classes (B)\n",
    "#  A number of features, 4 in this example\n",
    "#  B = 3 species of Iris (setosa, virginica and versicolor)\n",
    "A=data.shape[1]-1 # Number of features, Note first is y\n",
    "B=len(y_train_onehot[0])\n",
    "\n",
    "\n",
    "###########################################################\n",
    "#this works\n",
    "#x = tf.placeholder(tf.float32, name=\"x\", shape=[None, 4])\n",
    "#W = tf.Variable(tf.random_uniform([4, 3], -1, 1), name=\"W\")\n",
    "#b = tf.Variable(tf.zeros([3]), name=\"biases\")\n",
    "#output = tf.matmul(x, W) + b\n",
    "\n",
    "#init_op = tf.initialize_all_variables()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "#feed_dict = { x : x_train }\n",
    "#result = sess.run(output, feed_dict=feed_dict)\n",
    "#print result\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_stddev = (2.0/weight_shape[0])**0.5\n",
    "    w_init = tf.random_normal_initializer(stddev=weight_stddev)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
    "\n",
    "##########################################################\n",
    "#defines network architecture\n",
    "#deep neural network with 2 hidden layers\n",
    "\n",
    "def inference_deep(x, A, B):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [A, 4],[4])\n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [4, 4],[4])\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [4, B], [B])\n",
    "    return output\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def loss_deep(output, y):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(output, y)\n",
    "    loss = tf.reduce_mean(xentropy) \n",
    "    return loss\n",
    "\n",
    "\n",
    "###########################################################\n",
    "#defines the network architecture\n",
    "#simple logistic regression\n",
    "\n",
    "def inference(x, A, B):\n",
    "    W = tf.Variable(tf.zeros([A,B]))\n",
    "    b = tf.Variable(tf.zeros([B]))\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    return output\n",
    "   \n",
    "###########################################################\n",
    "\n",
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)#remove indices?\n",
    "    loss = tf.reduce_mean(xentropy) #remove this line?\n",
    "    return loss\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "def training(cost):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train_op = optimizer.minimize(cost)\n",
    "    return train_op\n",
    "\n",
    "###########################################################\n",
    "## add accuracy checking nodes\n",
    "\n",
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy\n",
    "\n",
    "###########################################################\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, A]) # Features\n",
    "y = tf.placeholder(\"float\", [None,B]) #correct label for x\n",
    "\n",
    "#output = inference_deep(x, A, B) ## for deep NN with 2 hidden layers\n",
    "#cost = loss_deep(output, y)\n",
    "\n",
    "output = inference(x, A, B) ## for logistic regression\n",
    "cost = loss(output, y)\n",
    "\n",
    "train_op = training(cost)\n",
    "eval_op = evaluate(output, y)\n",
    "\n",
    "##################################################################\n",
    "# Initialize and run\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print(\"...\")\n",
    "# Run the training\n",
    "for i in range(300):\n",
    "    sess.run(train_op, feed_dict={x: x_train, y: y_train_onehot})\n",
    "    result = sess.run(eval_op, feed_dict={x: x_test, y: y_test_onehot})\n",
    "    print \"Run {},{}\".format(i,result)\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print \"<<<<<<DONE>>>>>>\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
