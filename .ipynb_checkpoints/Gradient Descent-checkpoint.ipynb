{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사 하강법(Gradient Descent)으로 인자 찾아내기\n",
    "\n",
    "아래의 예제는 가중치 행렬W와 바이어스b를 경사하강법을 통해서 찾아내는 것을 보여줍니다. 목표값은 간단한 식으로 산출되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#numpy ran ->data \n",
    "x_data = np.float32(np.random.rand(2,100))\n",
    "#학습 레이블(목표값)은 아래의 식으로 산출.(w = [0.1,0.2], b = 0.3)\n",
    "y_data = np.dot([0.100,0.200], x_data) + 0.300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# float32\n",
    "float32\tSingle precision float: sign bit, 8 bits exponent, 23 bits mantissa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy.dot(a, b, out=None)\n",
    "\n",
    "dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
    "\n",
    "#Parameters\t\n",
    "\n",
    "a : array_like First argument.\n",
    "\n",
    "b : array_like Second argument.\n",
    "\n",
    "out : ndarray, optional\n",
    "Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.\n",
    "\n",
    "#Returns\n",
    "\n",
    "output : ndarray\n",
    "Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned.\n",
    "\n",
    "Raises:\t\n",
    "ValueError\n",
    "If the last dimension of a is not the same size as the second-to-last dimension of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터와 W,b를 사용해 선형모델을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = 0\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "# W는 1x2 형태의 weight variable (균등 랜덤값으로 초기화)\n",
    "w = tf.Variable(tf.random_uniform([1,2],-1.0,1.0))\n",
    "y = tf.matmul(w,x_data)+ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 손실과 학습 함수를 정의 합니다. 평균 제곱 오차(mean square error)가 최소화 되는 지점을 경사하강법(gradient descent)으로 구하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "#gradient descent, learning rate = 0.5\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "#learning operation \n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 세션을 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.89313352 -0.50347435]] [ 0.5088194]\n",
      "20 [[ 0.26414523  0.01270531]] [ 0.30335611]\n",
      "40 [[ 0.13928562  0.1561898 ]] [ 0.30026874]\n",
      "60 [[ 0.10940317  0.18975152]] [ 0.29993948]\n",
      "80 [[ 0.10225085  0.19760239]] [ 0.29995632]\n",
      "100 [[ 0.10053883  0.19943903]] [ 0.29998273]\n",
      "120 [[ 0.10012901  0.19986874]] [ 0.29999426]\n",
      "140 [[ 0.10003089  0.19996929]] [ 0.29999825]\n",
      "160 [[ 0.10000739  0.19999282]] [ 0.29999948]\n",
      "180 [[ 0.10000178  0.19999835]] [ 0.29999983]\n",
      "200 [[ 0.10000046  0.19999963]] [ 0.29999995]\n"
     ]
    }
   ],
   "source": [
    "# all variable init\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#start session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#learn 200 times\n",
    "for step in xrange(0,201) :\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0 :\n",
    "        print step, sess.run(w), sess.run(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
